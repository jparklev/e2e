# Conductor Sessions

AI conversation sessions using Claude (Opus) or OpenAI (GPT-5.2, Codex).

## Session Architecture

```
Workspace (e.g., "amarillo")
└── Session 1: "Implement smart contract" (gpt-5.2, 3106 msgs)
└── Session 2: "Explore Slither testing" (opus, 581 msgs)
└── Session 3: ...
```

## Models

| Model | Provider | Usage |
|-------|----------|-------|
| `opus` | Anthropic | Primary coding, complex reasoning |
| `gpt-5.2` | OpenAI | Code review, long sessions |
| `gpt-5.2-codex` | OpenAI | Codex-specific tasks |
| `gemini-3-flash` | Google | Research, online search (via CLI) |

## Session Status

| Status | Description |
|--------|-------------|
| `idle` | No active processing |
| `running` | Currently processing |
| `error` | Last operation failed |

## Message Format

Messages are stored as JSON in `session_messages.content`:

### User Message
```json
{
  "type": "user",
  "session_id": "019bb8ca-471d-76a2-ba3c-3c9ce69b5fca",
  "message": {
    "role": "user",
    "content": [
      {"type": "text", "text": "Please implement the smart contract..."}
    ]
  }
}
```

### Assistant Message
```json
{
  "type": "assistant",
  "session_id": "019bb8ca-471d-76a2-ba3c-3c9ce69b5fca",
  "message": {
    "role": "assistant",
    "content": [
      {"type": "text", "text": "I'll implement the contract..."}
    ]
  }
}
```

### Tool Use
```json
{
  "type": "assistant",
  "session_id": "...",
  "message": {
    "role": "assistant",
    "content": [
      {
        "type": "tool_use",
        "id": "item_189",
        "name": "Bash",
        "input": {"command": "git diff --stat"}
      }
    ]
  }
}
```

### Thinking Block
```json
{
  "type": "assistant",
  "session_id": "...",
  "message": {
    "role": "assistant",
    "content": [
      {
        "type": "thinking",
        "thinking": "**Analyzing the request**\n\nThe user wants..."
      }
    ]
  }
}
```

### Usage Stats
```json
{
  "type": "result",
  "session_id": "...",
  "usage": {
    "input_tokens": 99319,
    "output_tokens": 113,
    "cache_read_input_tokens": 4352
  }
}
```

## Example Session Flow

From the "Implement smart contract" session (3106 messages):

```
User: "please read this repo thoroughly, and @.context/smart-contract-scope.md.
       We want you to implement the smart contract plan... fork tests,
       mainnet state, arbitrum, base..."

User: "use gemini flash like `gemini -m gemini-3-flash-preview "prompt"`
       to search for things online..."

User: "we want to try to get all of these in: SlippagePostcheck,
       Morpho-specific health checks, Permit2, CoW swap"

User: "here's a review we received on this work..."
       [pastes security audit findings]

User: "Alright, we continue your work. Also, we made various other changes..."
```

## Session Queries

### Top sessions by message count
```sql
SELECT
    s.title,
    s.model,
    COUNT(sm.id) as msg_count
FROM sessions s
LEFT JOIN session_messages sm ON s.id = sm.session_id
GROUP BY s.id
ORDER BY msg_count DESC
LIMIT 15;
```

### Recent sessions with context usage
```sql
SELECT
    title,
    model,
    context_used_percent,
    status,
    created_at
FROM sessions
WHERE title != 'Untitled'
ORDER BY created_at DESC
LIMIT 20;
```

### Messages from specific session
```sql
SELECT
    role,
    substr(content, 1, 500) as preview,
    created_at
FROM session_messages
WHERE session_id = '<session_id>'
ORDER BY created_at;
```

## Multi-Model Collaboration

Sessions can invoke other models via CLI:

```bash
# Use Gemini for research
gemini -m gemini-3-flash-preview "what are the morpho protocol interfaces?"

# Use GPT for code review (configured as review_model)
# Automatically used when reviewing PRs
```

## Token Usage

Sessions track token consumption:
- `input_tokens`: Tokens sent to model
- `output_tokens`: Tokens generated by model
- `cache_read_input_tokens`: Tokens read from cache
- `context_used_percent`: Percentage of context window used (0-100)

Example from heavy session:
```
input_tokens: 7,827,477
output_tokens: 48,589
cache_read_input_tokens: 7,704,064
```
